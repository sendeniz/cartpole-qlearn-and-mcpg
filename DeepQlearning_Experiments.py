# -*- coding: utf-8 -*-
"""DQN_Experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-kdjPUr3oGS_cxIzmVGEMwLZR1ql4yPx
"""

!pip install keras-layer-normalization

import gym
import numpy as np
import math
import matplotlib.pyplot as plt
from collections import deque
import random
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras_layer_normalization import LayerNormalization
from keras.layers import Dropout
from keras import backend as K
import tensorflow as tf

"""## Architecture Experiments

Architecture a): One dense kayer 24 hidden Nodes
"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation):
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen = 2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = 'mse', optimizer = Adam(lr=self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   # returns action
  
  # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
  
  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')
# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32
reward_tracker = []
archi_res_1 = []
archi_1_last_100_rewards = deque(maxlen=100)
archi_1_avg_100_reward = []
archi_1_avg_reward = []

hidden_activation = "relu"
out_activation = "linear"
replay_switch = [True]
for on_off in range(len(replay_switch)):
  use_replay = replay_switch[on_off]
  agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation)
  for i_episode in range(n_episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    done = False
    rewards = 0
    time_steps_c = 0 
    while not done:
        # env.render()
        action = agent.pick_action(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -100   
        next_state = np.reshape(next_state, [1, state_size])

        # remember
        agent.store_transitions(state, action, reward, next_state, done)
        state = next_state
        rewards += 1
        time_steps_c += 1
        # for experience replay
        if len(agent.memory) > batch_size and use_replay == True:
            # use experience replay if memory length enough for batching
            agent.exp_replay(batch_size) 

    #print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
    reward_tracker.append(rewards)
    if use_replay == True :
      archi_res_1.append(rewards)
      archi_1_avg_reward.append(np.mean(archi_res_1))
      archi_1_last_100_rewards.append(rewards)
      archi_1_avg_100_reward.append(np.mean(archi_1_last_100_rewards))
  
    # stop training if agent solves environemnt for 25 sucessive episodes
    if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
        print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
        break

"""Architecture b): 2 Dense layers 24 hidden nodes"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation):
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      # using normalizaiton layer as input leads to performance degeneracies
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = 'mse', optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')
# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32

archi_res_2 = []
archi_2_last_100_rewards = deque(maxlen=100)
archi_2_avg_100_reward = []
archi_2_avg_reward = []

reward_tracker = []
hidden_activation = "relu"
out_activation = "linear"
replay_switch = [True]
for on_off in range(len(replay_switch)):
  use_replay = replay_switch[on_off]
  agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation)
  for i_episode in range(n_episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    done = False
    rewards = 0
    time_steps_c = 0 
    while not done:
        # env.render()
        action = agent.pick_action(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -100   
        next_state = np.reshape(next_state, [1, state_size])

        # remember
        agent.store_transitions(state, action, reward, next_state, done)
        state = next_state
        rewards += 1
        time_steps_c += 1
        # for experience replay
        if len(agent.memory) > batch_size and use_replay == True:
            # use experience replay if memory length enough for batching
            agent.exp_replay(batch_size) 
        #elif use_replay == False:
        
        #elif use_replay == False:
        #  agent.update_epsilon()

    #print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
    reward_tracker.append(rewards)
    if use_replay == True :
      archi_res_2.append(rewards)
      archi_2_avg_reward.append(np.mean(archi_res_2))
      archi_2_last_100_rewards.append(rewards)
      archi_2_avg_100_reward.append(np.mean(archi_2_last_100_rewards))
  
    # stop training if agent solves environemnt for 25 sucessive episodes
    if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
        print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
        break

"""Architecture c) 3 Dense layers each 24 hidden nodes"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation):
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   # returns action
  
  # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
  
  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')
# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32
archi_res_3 = []
archi_3_last_100_rewards = deque(maxlen=100)
archi_3_avg_100_reward = []
archi_3_avg_reward = []

reward_tracker = []
hidden_activation = "relu"
out_activation = "linear"
replay_switch = [True]
for on_off in range(len(replay_switch)):
  use_replay = replay_switch[on_off]
  agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation)
  for i_episode in range(n_episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
    done = False
    rewards = 0
    time_steps_c = 0 
    while not done:
        # env.render()
        action = agent.pick_action(state)
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -100   
        next_state = np.reshape(next_state, [1, state_size])

        # remember
        agent.store_transitions(state, action, reward, next_state, done)
        state = next_state
        rewards += 1
        time_steps_c += 1
        # for experience replay
        if len(agent.memory) > batch_size and use_replay == True:
            # use experience replay if memory length enough for batching
            agent.exp_replay(batch_size) 

    # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
    reward_tracker.append(rewards)
    if use_replay == True :
      archi_res_3.append(rewards)
      archi_3_avg_reward.append(np.mean(archi_res_3))
      archi_3_last_100_rewards.append(rewards)
      archi_3_avg_100_reward.append(np.mean(archi_3_last_100_rewards))
  
    # stop training if agent solves environemnt for 25 sucessive episodes
    if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
        print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
        break

plt.figure(figsize=(8, 6))
plt.plot(archi_res_1, linewidth=1.5) 
plt.plot(archi_res_2, linewidth=1.5)
plt.plot(archi_res_3, linewidth=1.5)
plt.title('DQN architectures: Total reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['1 hidden', '2 hidden', '3 hidden'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(archi_1_avg_reward, linewidth=1.5) # with experience replay Architecture 1
plt.plot(archi_2_avg_reward, linewidth=1.5)
plt.plot(archi_3_avg_reward, linewidth=1.5)
plt.title('DQN architectures: Mean reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Mean reward', fontsize = 18)
plt.legend(['1 hidden', '2 hidden', '3 hidden'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(archi_1_avg_100_reward, linewidth=1.5) # with experience replay Architecture 1
plt.plot(archi_2_avg_100_reward, linewidth=1.5)
plt.plot(archi_3_avg_100_reward, linewidth=1.5)
plt.title('DQN architectures: Mean last 100 rewards per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Mean reward', fontsize = 18)
plt.legend(['1 hidden', '2 hidden', '3 hidden'], 
            prop={'size': 16}, frameon=False)
plt.show()

"""## Replay On-Off Experiment"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation, loss):
        self.loss = loss
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = self.loss, optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')

# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32
res_rp_off = []
res_rp_on = [] # mse score result
res_avg_rp_off = []
res_avg_rp_on = []

reward_tracker = []
hidden_activation = "relu"
out_activation = "linear"
replay_switch = [False, True]
loss = 'mse'

for on_off in range(len(replay_switch)):
    use_replay = replay_switch[on_off]
    agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation, loss)
    for i_episode in range(n_episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        done = False
        rewards = 0
        time_steps_c = 0
        while not done:
            # env.render()
            action = agent.pick_action(state)
            next_state, reward, done, _ = env.step(action)
            reward = reward if not done else -100   
            next_state = np.reshape(next_state, [1, state_size])

            # remember
            agent.store_transitions(state, action, reward, next_state, done)
            state = next_state
            rewards += 1
            time_steps_c += 1
            # do experience replay if condition applies
            if len(agent.memory) > batch_size and use_replay == True:
                # use experience replay if memory length enough for batching
                history = agent.exp_replay(batch_size) 

        # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
        reward_tracker.append(rewards)
        if use_replay == False and agent.loss == 'mse':
            res_rp_off.append(rewards)
            res_avg_rp_off.append(np.mean(res_rp_off))
        elif use_replay == True and agent.loss == 'mse':
            res_rp_on.append(rewards)
            res_avg_rp_on.append(np.mean(res_rp_on))
      # stop training if agent solves environemnt for 25 sucessive episodes
        if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
            print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
            break

plt.figure(figsize=(8, 6))
plt.plot(res_rp_on, linewidth = 1.5) # with experience replay 
plt.plot(res_rp_off, linewidth = 1.5) # without experience replay
plt.title('DQN replay on/off: Total reward per episode', fontsize = 20)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['Replay on', 'Replay off'], 
            prop={'size': 16}, frameon=False)
plt.show()

"""## Activation Function Experiment"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation, loss, 
               kernel_init):
        self.kernel_init = kernel_init
        self.loss = loss
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation, 
                      kernel_initializer = self.kernel_init))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation, kernel_initializer = self.kernel_init))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation, kernel_initializer = self.kernel_init))
      model.summary()
      model.compile(loss = self.loss, optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')

# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32

res_relu = []
res_elu = [] 
res_sig = []
res_tanh = []

avg_relu = []
avg_elu = [] 
avg_sig = []
avg_tanh = []

last_100_relu = deque(maxlen=100)
last_100_elu = deque(maxlen=100)
last_100_sig = deque(maxlen=100)
last_100_tanh = deque(maxlen=100)

avg_last_100_relu = []
avg_last_100_elu = []
avg_last_100_sig = []
avg_last_100_tanh = []


activation_lst = ['relu', 'elu', 'sigmoid','tanh'] 
out_activation = "linear"
use_replay = True
loss = 'mse'
kernel_lst = ["he_uniform", "he_uniform", "glorot_uniform", "glorot_uniform"]

for kernels in range(len(kernel_lst)):
    reward_tracker = []
    kernel_init = kernel_lst[kernels]
    hidden_activation = activation_lst[kernels]
    agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, 
                     out_activation, loss, kernel_init)
    for i_episode in range(n_episodes):
        state = env.reset()
        state = np.reshape(state, [1, state_size])
        done = False
        rewards = 0
        time_steps_c = 0
        while not done:
            # env.render()
            action = agent.pick_action(state)
            next_state, reward, done, _ = env.step(action)
            reward = reward if not done else -100   
            next_state = np.reshape(next_state, [1, state_size])

            # remember
            agent.store_transitions(state, action, reward, next_state, done)
            state = next_state
            rewards += 1
            time_steps_c += 1
            # do experience replay if condition applies
            if len(agent.memory) > batch_size and use_replay == True:
                # use experience replay if memory length enough for batching
                history = agent.exp_replay(batch_size) 

        # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
        reward_tracker.append(rewards)
        if use_replay == True and agent.hidden_activation == 'relu':
            res_relu.append(rewards)
            avg_relu.append(np.mean(res_relu))
            last_100_relu.append(rewards)
            avg_last_100_relu.append(np.mean(last_100_relu))

        elif use_replay == True and agent.hidden_activation == 'elu':
            res_elu.append(rewards)
            avg_elu.append(np.mean(res_elu))
            last_100_elu.append(rewards)
            avg_last_100_elu.append(np.mean(last_100_elu))

        elif use_replay == True and agent.hidden_activation == 'sigmoid':
            res_sig.append(rewards)
            avg_sig.append(np.mean(res_sig))
            last_100_sig.append(rewards)
            avg_last_100_sig.append(np.mean(last_100_sig))
        
        elif use_replay == True and agent.hidden_activation == 'tanh':
            res_tanh.append(rewards)
            avg_tanh.append(np.mean(res_tanh))
            last_100_tanh.append(rewards)
            avg_last_100_tanh.append(np.mean(last_100_tanh))

      # stop training if agent solves environemnt for 25 sucessive episodes
        if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
            print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
            break

plt.figure(figsize=(8, 6))
plt.plot(res_relu, linewidth = 1.5) 
plt.plot(res_elu, linewidth = 1.5)
plt.plot(res_sig, linewidth = 1.5) 
plt.plot(res_tanh, linewidth = 1.5) 
plt.title('DQN activations: Total reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['relu', 'elu', 'sigmoid','tanh'], 
            prop={'size': 16}, frameon = False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(avg_relu, linewidth = 1.5) 
plt.plot(avg_elu, linewidth = 1.5)
plt.plot(avg_sig, linewidth = 1.5) 
plt.plot(avg_tanh, linewidth = 1.5) 
plt.title('DQN activations: Mean reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Mean reward', fontsize = 18)
plt.legend(['relu', 'elu', 'sigmoid','tanh'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(avg_last_100_relu, linewidth = 1.5) 
plt.plot(avg_last_100_elu, linewidth = 1.5)
plt.plot(avg_last_100_sig, linewidth = 1.5) 
plt.plot(avg_last_100_tanh, linewidth = 1.5) 
plt.title('DQN activations: Mean last 100 rewards per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Mean reward', fontsize = 18)
plt.legend(['relu', 'elu', 'sigmoid','tanh'], 
            prop={'size': 16}, frameon=False)
plt.show()

"""## Regularization Experiments"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation, loss):
        self.loss = loss
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))
      model.add(Dropout(rate=0.1))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation))
      model.add(Dropout(rate=0.1))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = self.loss, optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')

# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32

reward_tracker = []
hidden_activation = "sigmoid"
out_activation = "linear"
use_replay = True
loss = 'mse'
res_drop = []
avg_drop = []
last_100_drop = deque(maxlen=100)
avg_last_100_drop = []

agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation, loss)
for i_episode in range(n_episodes):
  state = env.reset()
  state = np.reshape(state, [1, state_size])
  done = False
  rewards = 0
  time_steps_c = 0
  while not done:
      # env.render()
      action = agent.pick_action(state)
      next_state, reward, done, _ = env.step(action)
      reward = reward if not done else -100   
      next_state = np.reshape(next_state, [1, state_size])

      # remember
      agent.store_transitions(state, action, reward, next_state, done)
      state = next_state
      rewards += 1
      time_steps_c += 1
      # do experience replay if condition applies
      if len(agent.memory) > batch_size and use_replay == True:
          # use experience replay if memory length enough for batching
          history = agent.exp_replay(batch_size) 

      # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
  reward_tracker.append(rewards)
  if use_replay == True and agent.loss == 'mse':
    res_drop.append(rewards)
    avg_drop.append(np.mean(res_drop))
    last_100_drop.append(rewards)
    avg_last_100_drop.append(np.mean(last_100_drop))

  # stop training if agent solves environemnt for 25 sucessive episodes
  if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
      print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
      break

"""L1 Regularization"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation, loss):
        self.loss = loss
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation,
                      kernel_regularizer=keras.regularizers.l2(l=0.1)))
      #model.add(Dropout(rate=0.1))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation,
                      kernel_regularizer=keras.regularizers.l2(l=0.1)))
      #model.add(Dropout(rate=0.1))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = self.loss, optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')

# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32

reward_tracker = []
hidden_activation = "sigmoid"

out_activation = "linear"
use_replay = True
loss = 'mse'
res_l1 = []
avg_l1 = []
last_100_l1 = deque(maxlen=100)
avg_last_100_l1 = []


agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation, loss)
for i_episode in range(n_episodes):
  state = env.reset()
  state = np.reshape(state, [1, state_size])
  done = False
  rewards = 0
  time_steps_c = 0
  while not done:
      # env.render()
      action = agent.pick_action(state)
      next_state, reward, done, _ = env.step(action)
      reward = reward if not done else -100   
      next_state = np.reshape(next_state, [1, state_size])

      # remember
      agent.store_transitions(state, action, reward, next_state, done)
      state = next_state
      rewards += 1
      time_steps_c += 1
      # do experience replay if condition applies
      if len(agent.memory) > batch_size and use_replay == True:
          # use experience replay if memory length enough for batching
          history = agent.exp_replay(batch_size) 

    # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
  reward_tracker.append(rewards)
  if use_replay == True and agent.loss == 'mse':
    res_l1.append(rewards)
    avg_l1.append(np.mean(res_l1))
    last_100_l1.append(rewards)
    avg_last_100_l1.append(np.mean(last_100_l1))

  # stop training if agent solves environemnt for 25 sucessive episodes
  if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
      print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
      break

"""L2 Regularization"""

class DQNAgent:
  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation, loss):
        self.loss = loss
        self.n_hidden = n_hidden
        self.hidden_activation = hidden_activation
        self.out_activation = out_activation
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        # reward discount rate
        self.gamma = 0.99    
        # exploration rate     
        self.epsilon = 1.0        
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.999
        self.learning_rate = 0.001
        self.min_memory_req = 1000
        self.model = self.create_nn()

  def create_nn(self):
      model = Sequential()
      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation,
                      kernel_regularizer=keras.regularizers.l2(l=0.1)))
      #model.add(Dropout(rate=0.2))
      model.add(LayerNormalization())
      model.add(Dense(n_hidden, activation = hidden_activation,
                      kernel_regularizer=keras.regularizers.l2(l=0.1)))
      #model.add(Dropout(rate=0.2))
      model.add(LayerNormalization())
      model.add(Dense(self.action_size, activation = out_activation))
      model.summary()
      model.compile(loss = self.loss, optimizer=Adam(lr = self.learning_rate))
      return model

  # store transition to allow dqn to remember
  def store_transitions(self, state, action, reward, next_state, done):
      self.memory.append((state, action, reward, next_state, done))

  def pick_action(self, state):
      if np.random.random() < self.epsilon:
          return random.randrange(self.action_size)
      else:
          q_values = self.model.predict(state)
          return np.argmax(q_values[0])   
  
    # experience replay buffer
  def exp_replay(self, batch_size):
    # randomly select a batch 
    minibatch = random.sample(self.memory, batch_size)
    # initalize current state
    state = np.zeros((batch_size, self.state_size))
    next_state = np.zeros((batch_size, self.state_size))
    action, reward, done = [], [], []

    for i in range(batch_size):
        state[i] = minibatch[i][0]  
        action.append(minibatch[i][1])
        reward.append(minibatch[i][2])
        next_state[i] = minibatch[i][3]  
        done.append(minibatch[i][4])

    # current state q value
    target_q_val = self.model.predict(state) 
    # compute next state q value
    q_val_next_state = self.model.predict(next_state)    

    for i in range(batch_size):
        if done[i]:
            target_q_val[i][action[i]] = reward[i]
        else:
            target_q_val[i][action[i]] = reward[i] + self.gamma * (
                np.amax(q_val_next_state[i]))

    # train model
    history = self.model.fit(state, target_q_val, batch_size=batch_size,
                   epochs=1, verbose=0)
    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay
    return history

  # update epsilon 
  def update_epsilon(self):
    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):
        self.epsilon *= self.epsilon_decay

# initialize the environment
env = gym.make('CartPole-v0')

# initalize parameters for DQN
state_size = env.observation_space.shape[0]
action_size = env.action_space.n 
n_hidden = 24
n_episodes = 200
done = False 
batch_size = 32

reward_tracker = []
hidden_activation = "sigmoid"

out_activation = "linear"
use_replay = True
loss = 'mse'
res_l2 = []
avg_l2 = []
last_100_l2 = deque(maxlen=100)
avg_last_100_l2 = []

agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation, loss)
for i_episode in range(n_episodes):
  state = env.reset()
  state = np.reshape(state, [1, state_size])
  done = False
  rewards = 0
  time_steps_c = 0
  while not done:
      # env.render()
      action = agent.pick_action(state)
      next_state, reward, done, _ = env.step(action)
      reward = reward if not done else -100   
      next_state = np.reshape(next_state, [1, state_size])

      # remember
      agent.store_transitions(state, action, reward, next_state, done)
      state = next_state
      rewards += 1
      time_steps_c += 1
      # do experience replay if condition applies
      if len(agent.memory) > batch_size and use_replay == True:
          # use experience replay if memory length enough for batching
          history = agent.exp_replay(batch_size) 

    # print('Episode {}\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))
  reward_tracker.append(rewards)
  if use_replay == True and agent.loss == 'mse':
    res_l2.append(rewards)
    avg_l2.append(np.mean(res_l2))
    last_100_l2.append(rewards)
    avg_last_100_l2.append(np.mean(last_100_l2))

  # stop training if agent solves environemnt for 25 sucessive episodes
  if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):
      print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))
      break

plt.figure(figsize=(8, 6))
plt.plot(res_sig, linewidth = 1.5) 
plt.plot(res_drop, linewidth = 1.5) 
plt.plot(res_l1, linewidth = 1.5) 
plt.plot(res_l2, linewidth = 1.5) 
plt.title('DQN Regularization: Total reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['current best','drop', 'l1', 'l2'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(avg_sig, linewidth = 1.5) 
plt.plot(avg_drop, linewidth = 1.5) 
plt.plot(avg_l1, linewidth = 1.5) 
plt.plot(avg_l2, linewidth = 1.5) 
plt.title('DQN Regularization: Mean reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['current best','drop', 'l1', 'l2'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(avg_last_100_sig, linewidth = 1.5) 
plt.plot(avg_last_100_drop, linewidth = 1.5) 
plt.plot(avg_last_100_l1, linewidth = 1.5) 
plt.plot(avg_last_100_l2, linewidth = 1.5) 
plt.title('DQN Regularization: Mean last 100 rewards per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['current best','drop', 'l1', 'l2'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(res_sig, linewidth = 1.5) 
plt.plot(learning_schedule_on, linewidth = 1.5) 
plt.title('Comparison Q-Table - DQN: Total reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['best dqn','q-table'], 
            prop={'size': 16}, frameon=False)
plt.show()

plt.figure(figsize=(8, 6))
plt.plot(avg_sig, linewidth = 1.5) 
plt.plot(avg_reward, linewidth = 1.5) 
plt.title('Comparison Q-Table - DQN: Mean reward per episode', fontsize = 18)
plt.xlabel('Number of episodes', fontsize = 18)
plt.ylabel('Total reward', fontsize = 18)
plt.legend(['best dqn','q-table'], 
            prop={'size': 16}, frameon=False)
plt.show()