{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_Clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzbVrO1ve6ib",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7a94eef5-30e2-4b37-dd0e-7d83f494791c"
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "class DQNAgent:\n",
        "  def __init__(self, state_size, action_size, n_hidden, hidden_activation, out_activation):\n",
        "        self.n_hidden = n_hidden\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.out_activation = out_activation\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        # reward discount rate\n",
        "        self.gamma = 0.99    \n",
        "        # exploration rate     \n",
        "        self.epsilon = 1.0        \n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.learning_rate = 0.001\n",
        "        self.min_memory_req = 1000\n",
        "        self.model = self.create_nn()\n",
        "\n",
        "  def create_nn(self):\n",
        "      model = Sequential()\n",
        "      model.add(Dense(n_hidden, input_dim=self.state_size, activation = hidden_activation))\n",
        "      model.add(Dense(n_hidden, activation = hidden_activation))\n",
        "      model.add(Dense(self.action_size, activation = out_activation))\n",
        "      model.summary()\n",
        "      model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "      return model\n",
        "\n",
        "  # store transition to allow dqn to remember\n",
        "  def store_transitions(self, state, action, reward, next_state, done):\n",
        "      self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "  def pick_action(self, state):\n",
        "      if np.random.random() < self.epsilon:\n",
        "          return random.randrange(self.action_size)\n",
        "      else:\n",
        "          q_values = self.model.predict(state)\n",
        "          return np.argmax(q_values[0])   # returns action\n",
        "  \n",
        "  # experience replay buffer\n",
        "  def exp_replay(self, batch_size):\n",
        "    # randomly select a batch \n",
        "    minibatch = random.sample(self.memory, batch_size)\n",
        "    # initalize current state\n",
        "    state = np.zeros((batch_size, self.state_size))\n",
        "    next_state = np.zeros((batch_size, self.state_size))\n",
        "    action, reward, done = [], [], []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        state[i] = minibatch[i][0]  \n",
        "        action.append(minibatch[i][1])\n",
        "        reward.append(minibatch[i][2])\n",
        "        next_state[i] = minibatch[i][3]  \n",
        "        done.append(minibatch[i][4])\n",
        "\n",
        "    # current state q value\n",
        "    target_q_val = self.model.predict(state) \n",
        "    # compute next state q value\n",
        "    q_val_next_state = self.model.predict(next_state)    \n",
        "\n",
        "    for i in range(batch_size):\n",
        "        if done[i]:\n",
        "            target_q_val[i][action[i]] = reward[i]\n",
        "        else:\n",
        "            target_q_val[i][action[i]] = reward[i] + self.gamma * (\n",
        "                np.amax(q_val_next_state[i]))\n",
        "\n",
        "    # train model\n",
        "    self.model.fit(state, target_q_val, batch_size=batch_size,\n",
        "                   epochs=1, verbose=0)\n",
        "    if (self.epsilon > self.epsilon_min) and (len(self.memory) >= self.min_memory_req):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "  \n",
        "  # update epsilon \n",
        "  def update_epsilon(self):\n",
        "    if self.epsilon > self.epsilon_min and (len(self.memory) >= self.min_memory_req):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# initialize the environment\n",
        "env = gym.make('CartPole-v0')\n",
        "\n",
        "# initalize parameters for DQN\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n \n",
        "n_hidden = 24\n",
        "n_episodes = 200\n",
        "done = False \n",
        "batch_size = 32\n",
        "res_lst_1 = []\n",
        "res_lst_2 = []\n",
        "reward_tracker = []\n",
        "hidden_activation = \"relu\"\n",
        "out_activation = \"linear\"\n",
        "replay_switch = [False, True]\n",
        "for on_off in range(len(replay_switch)):\n",
        "  use_replay = replay_switch[on_off]\n",
        "  agent = DQNAgent(state_size, action_size, n_hidden, hidden_activation, out_activation)\n",
        "  for i_episode in range(n_episodes):\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    done = False\n",
        "    rewards = 0\n",
        "    time_steps_c = 0 \n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = agent.pick_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        reward = reward if not done else -100   \n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "        # remember\n",
        "        agent.store_transitions(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        rewards += 1\n",
        "        time_steps_c += 1\n",
        "        # for experience replay\n",
        "        if len(agent.memory) > batch_size and use_replay == True:\n",
        "            # use experience replay if memory length enough for batching\n",
        "            agent.exp_replay(batch_size) \n",
        "        #elif use_replay == False:\n",
        "        \n",
        "        elif use_replay == False:\n",
        "          agent.update_epsilon()\n",
        "\n",
        "    print('Episode {}\\{}: done after {} timesteps, total rewards {}, exploration {:.2}, memory length {}'.format(i_episode, n_episodes, time_steps_c, rewards, agent.epsilon, len(agent.memory)))\n",
        "    reward_tracker.append(rewards)\n",
        "    if use_replay == True :\n",
        "      res_lst_1.append(rewards)\n",
        "    elif use_replay == False:\n",
        "      res_lst_2.append(rewards)\n",
        "\n",
        "    # stop training if agent solves environemnt for 25 sucessive episodes\n",
        "    if np.mean(reward_tracker[-min(25, len(reward_tracker)):]) >= (env.spec.max_episode_steps - 5):\n",
        "        print('Agent consequitively solved the game with a score => 195 in {} episodes'.format(i_episode))\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode 0\\200: done after 38 timesteps, total rewards 38, exploration 1.0, memory length 38\n",
            "Episode 1\\200: done after 16 timesteps, total rewards 16, exploration 1.0, memory length 54\n",
            "Episode 2\\200: done after 21 timesteps, total rewards 21, exploration 1.0, memory length 75\n",
            "Episode 3\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 89\n",
            "Episode 4\\200: done after 18 timesteps, total rewards 18, exploration 1.0, memory length 107\n",
            "Episode 5\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 121\n",
            "Episode 6\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 134\n",
            "Episode 7\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 153\n",
            "Episode 8\\200: done after 18 timesteps, total rewards 18, exploration 1.0, memory length 171\n",
            "Episode 9\\200: done after 12 timesteps, total rewards 12, exploration 1.0, memory length 183\n",
            "Episode 10\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 202\n",
            "Episode 11\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 217\n",
            "Episode 12\\200: done after 20 timesteps, total rewards 20, exploration 1.0, memory length 237\n",
            "Episode 13\\200: done after 10 timesteps, total rewards 10, exploration 1.0, memory length 247\n",
            "Episode 14\\200: done after 23 timesteps, total rewards 23, exploration 1.0, memory length 270\n",
            "Episode 15\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 283\n",
            "Episode 16\\200: done after 29 timesteps, total rewards 29, exploration 1.0, memory length 312\n",
            "Episode 17\\200: done after 18 timesteps, total rewards 18, exploration 1.0, memory length 330\n",
            "Episode 18\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 345\n",
            "Episode 19\\200: done after 40 timesteps, total rewards 40, exploration 1.0, memory length 385\n",
            "Episode 20\\200: done after 79 timesteps, total rewards 79, exploration 1.0, memory length 464\n",
            "Episode 21\\200: done after 17 timesteps, total rewards 17, exploration 1.0, memory length 481\n",
            "Episode 22\\200: done after 35 timesteps, total rewards 35, exploration 1.0, memory length 516\n",
            "Episode 23\\200: done after 51 timesteps, total rewards 51, exploration 1.0, memory length 567\n",
            "Episode 24\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 582\n",
            "Episode 25\\200: done after 48 timesteps, total rewards 48, exploration 1.0, memory length 630\n",
            "Episode 26\\200: done after 32 timesteps, total rewards 32, exploration 1.0, memory length 662\n",
            "Episode 27\\200: done after 30 timesteps, total rewards 30, exploration 1.0, memory length 692\n",
            "Episode 28\\200: done after 12 timesteps, total rewards 12, exploration 1.0, memory length 704\n",
            "Episode 29\\200: done after 25 timesteps, total rewards 25, exploration 1.0, memory length 729\n",
            "Episode 30\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 744\n",
            "Episode 31\\200: done after 11 timesteps, total rewards 11, exploration 1.0, memory length 755\n",
            "Episode 32\\200: done after 32 timesteps, total rewards 32, exploration 1.0, memory length 787\n",
            "Episode 33\\200: done after 18 timesteps, total rewards 18, exploration 1.0, memory length 805\n",
            "Episode 34\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 820\n",
            "Episode 35\\200: done after 76 timesteps, total rewards 76, exploration 1.0, memory length 896\n",
            "Episode 36\\200: done after 23 timesteps, total rewards 23, exploration 1.0, memory length 919\n",
            "Episode 37\\200: done after 26 timesteps, total rewards 26, exploration 1.0, memory length 945\n",
            "Episode 38\\200: done after 36 timesteps, total rewards 36, exploration 1.0, memory length 981\n",
            "Episode 39\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 1000\n",
            "Episode 40\\200: done after 11 timesteps, total rewards 11, exploration 0.99, memory length 1011\n",
            "Episode 41\\200: done after 12 timesteps, total rewards 12, exploration 0.98, memory length 1023\n",
            "Episode 42\\200: done after 23 timesteps, total rewards 23, exploration 0.95, memory length 1046\n",
            "Episode 43\\200: done after 62 timesteps, total rewards 62, exploration 0.9, memory length 1108\n",
            "Episode 44\\200: done after 39 timesteps, total rewards 39, exploration 0.86, memory length 1147\n",
            "Episode 45\\200: done after 24 timesteps, total rewards 24, exploration 0.84, memory length 1171\n",
            "Episode 46\\200: done after 14 timesteps, total rewards 14, exploration 0.83, memory length 1185\n",
            "Episode 47\\200: done after 18 timesteps, total rewards 18, exploration 0.82, memory length 1203\n",
            "Episode 48\\200: done after 18 timesteps, total rewards 18, exploration 0.8, memory length 1221\n",
            "Episode 49\\200: done after 9 timesteps, total rewards 9, exploration 0.79, memory length 1230\n",
            "Episode 50\\200: done after 28 timesteps, total rewards 28, exploration 0.77, memory length 1258\n",
            "Episode 51\\200: done after 58 timesteps, total rewards 58, exploration 0.73, memory length 1316\n",
            "Episode 52\\200: done after 40 timesteps, total rewards 40, exploration 0.7, memory length 1356\n",
            "Episode 53\\200: done after 36 timesteps, total rewards 36, exploration 0.67, memory length 1392\n",
            "Episode 54\\200: done after 24 timesteps, total rewards 24, exploration 0.66, memory length 1416\n",
            "Episode 55\\200: done after 31 timesteps, total rewards 31, exploration 0.64, memory length 1447\n",
            "Episode 56\\200: done after 32 timesteps, total rewards 32, exploration 0.62, memory length 1479\n",
            "Episode 57\\200: done after 40 timesteps, total rewards 40, exploration 0.59, memory length 1519\n",
            "Episode 58\\200: done after 40 timesteps, total rewards 40, exploration 0.57, memory length 1559\n",
            "Episode 59\\200: done after 59 timesteps, total rewards 59, exploration 0.54, memory length 1618\n",
            "Episode 60\\200: done after 35 timesteps, total rewards 35, exploration 0.52, memory length 1653\n",
            "Episode 61\\200: done after 20 timesteps, total rewards 20, exploration 0.51, memory length 1673\n",
            "Episode 62\\200: done after 47 timesteps, total rewards 47, exploration 0.49, memory length 1720\n",
            "Episode 63\\200: done after 15 timesteps, total rewards 15, exploration 0.48, memory length 1735\n",
            "Episode 64\\200: done after 36 timesteps, total rewards 36, exploration 0.46, memory length 1771\n",
            "Episode 65\\200: done after 59 timesteps, total rewards 59, exploration 0.44, memory length 1830\n",
            "Episode 66\\200: done after 18 timesteps, total rewards 18, exploration 0.43, memory length 1848\n",
            "Episode 67\\200: done after 38 timesteps, total rewards 38, exploration 0.41, memory length 1886\n",
            "Episode 68\\200: done after 56 timesteps, total rewards 56, exploration 0.39, memory length 1942\n",
            "Episode 69\\200: done after 46 timesteps, total rewards 46, exploration 0.37, memory length 1988\n",
            "Episode 70\\200: done after 61 timesteps, total rewards 61, exploration 0.35, memory length 2000\n",
            "Episode 71\\200: done after 52 timesteps, total rewards 52, exploration 0.33, memory length 2000\n",
            "Episode 72\\200: done after 31 timesteps, total rewards 31, exploration 0.32, memory length 2000\n",
            "Episode 73\\200: done after 43 timesteps, total rewards 43, exploration 0.31, memory length 2000\n",
            "Episode 74\\200: done after 24 timesteps, total rewards 24, exploration 0.3, memory length 2000\n",
            "Episode 75\\200: done after 51 timesteps, total rewards 51, exploration 0.29, memory length 2000\n",
            "Episode 76\\200: done after 33 timesteps, total rewards 33, exploration 0.28, memory length 2000\n",
            "Episode 77\\200: done after 18 timesteps, total rewards 18, exploration 0.27, memory length 2000\n",
            "Episode 78\\200: done after 38 timesteps, total rewards 38, exploration 0.26, memory length 2000\n",
            "Episode 79\\200: done after 38 timesteps, total rewards 38, exploration 0.25, memory length 2000\n",
            "Episode 80\\200: done after 18 timesteps, total rewards 18, exploration 0.25, memory length 2000\n",
            "Episode 81\\200: done after 33 timesteps, total rewards 33, exploration 0.24, memory length 2000\n",
            "Episode 82\\200: done after 47 timesteps, total rewards 47, exploration 0.23, memory length 2000\n",
            "Episode 83\\200: done after 32 timesteps, total rewards 32, exploration 0.22, memory length 2000\n",
            "Episode 84\\200: done after 34 timesteps, total rewards 34, exploration 0.21, memory length 2000\n",
            "Episode 85\\200: done after 28 timesteps, total rewards 28, exploration 0.21, memory length 2000\n",
            "Episode 86\\200: done after 33 timesteps, total rewards 33, exploration 0.2, memory length 2000\n",
            "Episode 87\\200: done after 58 timesteps, total rewards 58, exploration 0.19, memory length 2000\n",
            "Episode 88\\200: done after 44 timesteps, total rewards 44, exploration 0.18, memory length 2000\n",
            "Episode 89\\200: done after 60 timesteps, total rewards 60, exploration 0.17, memory length 2000\n",
            "Episode 90\\200: done after 35 timesteps, total rewards 35, exploration 0.17, memory length 2000\n",
            "Episode 91\\200: done after 75 timesteps, total rewards 75, exploration 0.15, memory length 2000\n",
            "Episode 92\\200: done after 17 timesteps, total rewards 17, exploration 0.15, memory length 2000\n",
            "Episode 93\\200: done after 55 timesteps, total rewards 55, exploration 0.14, memory length 2000\n",
            "Episode 94\\200: done after 91 timesteps, total rewards 91, exploration 0.13, memory length 2000\n",
            "Episode 95\\200: done after 55 timesteps, total rewards 55, exploration 0.12, memory length 2000\n",
            "Episode 96\\200: done after 27 timesteps, total rewards 27, exploration 0.12, memory length 2000\n",
            "Episode 97\\200: done after 27 timesteps, total rewards 27, exploration 0.12, memory length 2000\n",
            "Episode 98\\200: done after 18 timesteps, total rewards 18, exploration 0.11, memory length 2000\n",
            "Episode 99\\200: done after 31 timesteps, total rewards 31, exploration 0.11, memory length 2000\n",
            "Episode 100\\200: done after 33 timesteps, total rewards 33, exploration 0.11, memory length 2000\n",
            "Episode 101\\200: done after 51 timesteps, total rewards 51, exploration 0.1, memory length 2000\n",
            "Episode 102\\200: done after 31 timesteps, total rewards 31, exploration 0.099, memory length 2000\n",
            "Episode 103\\200: done after 40 timesteps, total rewards 40, exploration 0.095, memory length 2000\n",
            "Episode 104\\200: done after 28 timesteps, total rewards 28, exploration 0.093, memory length 2000\n",
            "Episode 105\\200: done after 42 timesteps, total rewards 42, exploration 0.089, memory length 2000\n",
            "Episode 106\\200: done after 23 timesteps, total rewards 23, exploration 0.087, memory length 2000\n",
            "Episode 107\\200: done after 33 timesteps, total rewards 33, exploration 0.084, memory length 2000\n",
            "Episode 108\\200: done after 31 timesteps, total rewards 31, exploration 0.081, memory length 2000\n",
            "Episode 109\\200: done after 16 timesteps, total rewards 16, exploration 0.08, memory length 2000\n",
            "Episode 110\\200: done after 37 timesteps, total rewards 37, exploration 0.077, memory length 2000\n",
            "Episode 111\\200: done after 25 timesteps, total rewards 25, exploration 0.075, memory length 2000\n",
            "Episode 112\\200: done after 57 timesteps, total rewards 57, exploration 0.071, memory length 2000\n",
            "Episode 113\\200: done after 30 timesteps, total rewards 30, exploration 0.069, memory length 2000\n",
            "Episode 114\\200: done after 35 timesteps, total rewards 35, exploration 0.067, memory length 2000\n",
            "Episode 115\\200: done after 24 timesteps, total rewards 24, exploration 0.065, memory length 2000\n",
            "Episode 116\\200: done after 38 timesteps, total rewards 38, exploration 0.063, memory length 2000\n",
            "Episode 117\\200: done after 31 timesteps, total rewards 31, exploration 0.061, memory length 2000\n",
            "Episode 118\\200: done after 23 timesteps, total rewards 23, exploration 0.059, memory length 2000\n",
            "Episode 119\\200: done after 28 timesteps, total rewards 28, exploration 0.058, memory length 2000\n",
            "Episode 120\\200: done after 27 timesteps, total rewards 27, exploration 0.056, memory length 2000\n",
            "Episode 121\\200: done after 26 timesteps, total rewards 26, exploration 0.055, memory length 2000\n",
            "Episode 122\\200: done after 23 timesteps, total rewards 23, exploration 0.053, memory length 2000\n",
            "Episode 123\\200: done after 29 timesteps, total rewards 29, exploration 0.052, memory length 2000\n",
            "Episode 124\\200: done after 22 timesteps, total rewards 22, exploration 0.051, memory length 2000\n",
            "Episode 125\\200: done after 15 timesteps, total rewards 15, exploration 0.05, memory length 2000\n",
            "Episode 126\\200: done after 24 timesteps, total rewards 24, exploration 0.049, memory length 2000\n",
            "Episode 127\\200: done after 26 timesteps, total rewards 26, exploration 0.048, memory length 2000\n",
            "Episode 128\\200: done after 18 timesteps, total rewards 18, exploration 0.047, memory length 2000\n",
            "Episode 129\\200: done after 29 timesteps, total rewards 29, exploration 0.045, memory length 2000\n",
            "Episode 130\\200: done after 18 timesteps, total rewards 18, exploration 0.045, memory length 2000\n",
            "Episode 131\\200: done after 23 timesteps, total rewards 23, exploration 0.044, memory length 2000\n",
            "Episode 132\\200: done after 43 timesteps, total rewards 43, exploration 0.042, memory length 2000\n",
            "Episode 133\\200: done after 33 timesteps, total rewards 33, exploration 0.04, memory length 2000\n",
            "Episode 134\\200: done after 22 timesteps, total rewards 22, exploration 0.039, memory length 2000\n",
            "Episode 135\\200: done after 41 timesteps, total rewards 41, exploration 0.038, memory length 2000\n",
            "Episode 136\\200: done after 39 timesteps, total rewards 39, exploration 0.036, memory length 2000\n",
            "Episode 137\\200: done after 42 timesteps, total rewards 42, exploration 0.035, memory length 2000\n",
            "Episode 138\\200: done after 91 timesteps, total rewards 91, exploration 0.032, memory length 2000\n",
            "Episode 139\\200: done after 35 timesteps, total rewards 35, exploration 0.031, memory length 2000\n",
            "Episode 140\\200: done after 87 timesteps, total rewards 87, exploration 0.028, memory length 2000\n",
            "Episode 141\\200: done after 19 timesteps, total rewards 19, exploration 0.028, memory length 2000\n",
            "Episode 142\\200: done after 35 timesteps, total rewards 35, exploration 0.027, memory length 2000\n",
            "Episode 143\\200: done after 30 timesteps, total rewards 30, exploration 0.026, memory length 2000\n",
            "Episode 144\\200: done after 67 timesteps, total rewards 67, exploration 0.024, memory length 2000\n",
            "Episode 145\\200: done after 32 timesteps, total rewards 32, exploration 0.024, memory length 2000\n",
            "Episode 146\\200: done after 23 timesteps, total rewards 23, exploration 0.023, memory length 2000\n",
            "Episode 147\\200: done after 74 timesteps, total rewards 74, exploration 0.021, memory length 2000\n",
            "Episode 148\\200: done after 69 timesteps, total rewards 69, exploration 0.02, memory length 2000\n",
            "Episode 149\\200: done after 23 timesteps, total rewards 23, exploration 0.019, memory length 2000\n",
            "Episode 150\\200: done after 30 timesteps, total rewards 30, exploration 0.019, memory length 2000\n",
            "Episode 151\\200: done after 55 timesteps, total rewards 55, exploration 0.018, memory length 2000\n",
            "Episode 152\\200: done after 77 timesteps, total rewards 77, exploration 0.017, memory length 2000\n",
            "Episode 153\\200: done after 101 timesteps, total rewards 101, exploration 0.015, memory length 2000\n",
            "Episode 154\\200: done after 21 timesteps, total rewards 21, exploration 0.015, memory length 2000\n",
            "Episode 155\\200: done after 21 timesteps, total rewards 21, exploration 0.014, memory length 2000\n",
            "Episode 156\\200: done after 63 timesteps, total rewards 63, exploration 0.013, memory length 2000\n",
            "Episode 157\\200: done after 57 timesteps, total rewards 57, exploration 0.013, memory length 2000\n",
            "Episode 158\\200: done after 23 timesteps, total rewards 23, exploration 0.012, memory length 2000\n",
            "Episode 159\\200: done after 45 timesteps, total rewards 45, exploration 0.012, memory length 2000\n",
            "Episode 160\\200: done after 19 timesteps, total rewards 19, exploration 0.012, memory length 2000\n",
            "Episode 161\\200: done after 57 timesteps, total rewards 57, exploration 0.011, memory length 2000\n",
            "Episode 162\\200: done after 17 timesteps, total rewards 17, exploration 0.011, memory length 2000\n",
            "Episode 163\\200: done after 97 timesteps, total rewards 97, exploration 0.01, memory length 2000\n",
            "Episode 164\\200: done after 37 timesteps, total rewards 37, exploration 0.01, memory length 2000\n",
            "Episode 165\\200: done after 28 timesteps, total rewards 28, exploration 0.01, memory length 2000\n",
            "Episode 166\\200: done after 34 timesteps, total rewards 34, exploration 0.01, memory length 2000\n",
            "Episode 167\\200: done after 71 timesteps, total rewards 71, exploration 0.01, memory length 2000\n",
            "Episode 168\\200: done after 65 timesteps, total rewards 65, exploration 0.01, memory length 2000\n",
            "Episode 169\\200: done after 39 timesteps, total rewards 39, exploration 0.01, memory length 2000\n",
            "Episode 170\\200: done after 27 timesteps, total rewards 27, exploration 0.01, memory length 2000\n",
            "Episode 171\\200: done after 25 timesteps, total rewards 25, exploration 0.01, memory length 2000\n",
            "Episode 172\\200: done after 35 timesteps, total rewards 35, exploration 0.01, memory length 2000\n",
            "Episode 173\\200: done after 35 timesteps, total rewards 35, exploration 0.01, memory length 2000\n",
            "Episode 174\\200: done after 31 timesteps, total rewards 31, exploration 0.01, memory length 2000\n",
            "Episode 175\\200: done after 58 timesteps, total rewards 58, exploration 0.01, memory length 2000\n",
            "Episode 176\\200: done after 21 timesteps, total rewards 21, exploration 0.01, memory length 2000\n",
            "Episode 177\\200: done after 70 timesteps, total rewards 70, exploration 0.01, memory length 2000\n",
            "Episode 178\\200: done after 55 timesteps, total rewards 55, exploration 0.01, memory length 2000\n",
            "Episode 179\\200: done after 33 timesteps, total rewards 33, exploration 0.01, memory length 2000\n",
            "Episode 180\\200: done after 31 timesteps, total rewards 31, exploration 0.01, memory length 2000\n",
            "Episode 181\\200: done after 18 timesteps, total rewards 18, exploration 0.01, memory length 2000\n",
            "Episode 182\\200: done after 28 timesteps, total rewards 28, exploration 0.01, memory length 2000\n",
            "Episode 183\\200: done after 41 timesteps, total rewards 41, exploration 0.01, memory length 2000\n",
            "Episode 184\\200: done after 101 timesteps, total rewards 101, exploration 0.01, memory length 2000\n",
            "Episode 185\\200: done after 25 timesteps, total rewards 25, exploration 0.01, memory length 2000\n",
            "Episode 186\\200: done after 29 timesteps, total rewards 29, exploration 0.01, memory length 2000\n",
            "Episode 187\\200: done after 30 timesteps, total rewards 30, exploration 0.01, memory length 2000\n",
            "Episode 188\\200: done after 23 timesteps, total rewards 23, exploration 0.01, memory length 2000\n",
            "Episode 189\\200: done after 39 timesteps, total rewards 39, exploration 0.01, memory length 2000\n",
            "Episode 190\\200: done after 24 timesteps, total rewards 24, exploration 0.01, memory length 2000\n",
            "Episode 191\\200: done after 22 timesteps, total rewards 22, exploration 0.01, memory length 2000\n",
            "Episode 192\\200: done after 14 timesteps, total rewards 14, exploration 0.01, memory length 2000\n",
            "Episode 193\\200: done after 32 timesteps, total rewards 32, exploration 0.01, memory length 2000\n",
            "Episode 194\\200: done after 50 timesteps, total rewards 50, exploration 0.01, memory length 2000\n",
            "Episode 195\\200: done after 47 timesteps, total rewards 47, exploration 0.01, memory length 2000\n",
            "Episode 196\\200: done after 66 timesteps, total rewards 66, exploration 0.01, memory length 2000\n",
            "Episode 197\\200: done after 31 timesteps, total rewards 31, exploration 0.01, memory length 2000\n",
            "Episode 198\\200: done after 25 timesteps, total rewards 25, exploration 0.01, memory length 2000\n",
            "Episode 199\\200: done after 20 timesteps, total rewards 20, exploration 0.01, memory length 2000\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 24)                120       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 24)                600       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 50        \n",
            "=================================================================\n",
            "Total params: 770\n",
            "Trainable params: 770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Episode 0\\200: done after 28 timesteps, total rewards 28, exploration 1.0, memory length 28\n",
            "Episode 1\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 41\n",
            "Episode 2\\200: done after 28 timesteps, total rewards 28, exploration 1.0, memory length 69\n",
            "Episode 3\\200: done after 33 timesteps, total rewards 33, exploration 1.0, memory length 102\n",
            "Episode 4\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 117\n",
            "Episode 5\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 130\n",
            "Episode 6\\200: done after 8 timesteps, total rewards 8, exploration 1.0, memory length 138\n",
            "Episode 7\\200: done after 27 timesteps, total rewards 27, exploration 1.0, memory length 165\n",
            "Episode 8\\200: done after 21 timesteps, total rewards 21, exploration 1.0, memory length 186\n",
            "Episode 9\\200: done after 20 timesteps, total rewards 20, exploration 1.0, memory length 206\n",
            "Episode 10\\200: done after 30 timesteps, total rewards 30, exploration 1.0, memory length 236\n",
            "Episode 11\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 249\n",
            "Episode 12\\200: done after 11 timesteps, total rewards 11, exploration 1.0, memory length 260\n",
            "Episode 13\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 274\n",
            "Episode 14\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 289\n",
            "Episode 15\\200: done after 23 timesteps, total rewards 23, exploration 1.0, memory length 312\n",
            "Episode 16\\200: done after 40 timesteps, total rewards 40, exploration 1.0, memory length 352\n",
            "Episode 17\\200: done after 30 timesteps, total rewards 30, exploration 1.0, memory length 382\n",
            "Episode 18\\200: done after 16 timesteps, total rewards 16, exploration 1.0, memory length 398\n",
            "Episode 19\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 412\n",
            "Episode 20\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 427\n",
            "Episode 21\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 441\n",
            "Episode 22\\200: done after 27 timesteps, total rewards 27, exploration 1.0, memory length 468\n",
            "Episode 23\\200: done after 18 timesteps, total rewards 18, exploration 1.0, memory length 486\n",
            "Episode 24\\200: done after 23 timesteps, total rewards 23, exploration 1.0, memory length 509\n",
            "Episode 25\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 528\n",
            "Episode 26\\200: done after 24 timesteps, total rewards 24, exploration 1.0, memory length 552\n",
            "Episode 27\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 565\n",
            "Episode 28\\200: done after 11 timesteps, total rewards 11, exploration 1.0, memory length 576\n",
            "Episode 29\\200: done after 26 timesteps, total rewards 26, exploration 1.0, memory length 602\n",
            "Episode 30\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 621\n",
            "Episode 31\\200: done after 10 timesteps, total rewards 10, exploration 1.0, memory length 631\n",
            "Episode 32\\200: done after 40 timesteps, total rewards 40, exploration 1.0, memory length 671\n",
            "Episode 33\\200: done after 17 timesteps, total rewards 17, exploration 1.0, memory length 688\n",
            "Episode 34\\200: done after 13 timesteps, total rewards 13, exploration 1.0, memory length 701\n",
            "Episode 35\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 716\n",
            "Episode 36\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 731\n",
            "Episode 37\\200: done after 20 timesteps, total rewards 20, exploration 1.0, memory length 751\n",
            "Episode 38\\200: done after 15 timesteps, total rewards 15, exploration 1.0, memory length 766\n",
            "Episode 39\\200: done after 19 timesteps, total rewards 19, exploration 1.0, memory length 785\n",
            "Episode 40\\200: done after 23 timesteps, total rewards 23, exploration 1.0, memory length 808\n",
            "Episode 41\\200: done after 34 timesteps, total rewards 34, exploration 1.0, memory length 842\n",
            "Episode 42\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 856\n",
            "Episode 43\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 870\n",
            "Episode 44\\200: done after 17 timesteps, total rewards 17, exploration 1.0, memory length 887\n",
            "Episode 45\\200: done after 14 timesteps, total rewards 14, exploration 1.0, memory length 901\n",
            "Episode 46\\200: done after 33 timesteps, total rewards 33, exploration 1.0, memory length 934\n",
            "Episode 47\\200: done after 9 timesteps, total rewards 9, exploration 1.0, memory length 943\n",
            "Episode 48\\200: done after 38 timesteps, total rewards 38, exploration 1.0, memory length 981\n",
            "Episode 49\\200: done after 10 timesteps, total rewards 10, exploration 1.0, memory length 991\n",
            "Episode 50\\200: done after 22 timesteps, total rewards 22, exploration 0.99, memory length 1013\n",
            "Episode 51\\200: done after 47 timesteps, total rewards 47, exploration 0.94, memory length 1060\n",
            "Episode 52\\200: done after 11 timesteps, total rewards 11, exploration 0.93, memory length 1071\n",
            "Episode 53\\200: done after 12 timesteps, total rewards 12, exploration 0.92, memory length 1083\n",
            "Episode 54\\200: done after 25 timesteps, total rewards 25, exploration 0.9, memory length 1108\n",
            "Episode 55\\200: done after 42 timesteps, total rewards 42, exploration 0.86, memory length 1150\n",
            "Episode 56\\200: done after 36 timesteps, total rewards 36, exploration 0.83, memory length 1186\n",
            "Episode 57\\200: done after 15 timesteps, total rewards 15, exploration 0.82, memory length 1201\n",
            "Episode 58\\200: done after 19 timesteps, total rewards 19, exploration 0.8, memory length 1220\n",
            "Episode 59\\200: done after 50 timesteps, total rewards 50, exploration 0.76, memory length 1270\n",
            "Episode 60\\200: done after 77 timesteps, total rewards 77, exploration 0.71, memory length 1347\n",
            "Episode 61\\200: done after 40 timesteps, total rewards 40, exploration 0.68, memory length 1387\n",
            "Episode 62\\200: done after 52 timesteps, total rewards 52, exploration 0.64, memory length 1439\n",
            "Episode 63\\200: done after 125 timesteps, total rewards 125, exploration 0.57, memory length 1564\n",
            "Episode 64\\200: done after 75 timesteps, total rewards 75, exploration 0.53, memory length 1639\n",
            "Episode 65\\200: done after 63 timesteps, total rewards 63, exploration 0.49, memory length 1702\n",
            "Episode 66\\200: done after 40 timesteps, total rewards 40, exploration 0.48, memory length 1742\n",
            "Episode 67\\200: done after 80 timesteps, total rewards 80, exploration 0.44, memory length 1822\n",
            "Episode 68\\200: done after 64 timesteps, total rewards 64, exploration 0.41, memory length 1886\n",
            "Episode 69\\200: done after 85 timesteps, total rewards 85, exploration 0.38, memory length 1971\n",
            "Episode 70\\200: done after 26 timesteps, total rewards 26, exploration 0.37, memory length 1997\n",
            "Episode 71\\200: done after 63 timesteps, total rewards 63, exploration 0.35, memory length 2000\n",
            "Episode 72\\200: done after 87 timesteps, total rewards 87, exploration 0.32, memory length 2000\n",
            "Episode 73\\200: done after 104 timesteps, total rewards 104, exploration 0.29, memory length 2000\n",
            "Episode 74\\200: done after 128 timesteps, total rewards 128, exploration 0.25, memory length 2000\n",
            "Episode 75\\200: done after 72 timesteps, total rewards 72, exploration 0.23, memory length 2000\n",
            "Episode 76\\200: done after 91 timesteps, total rewards 91, exploration 0.21, memory length 2000\n",
            "Episode 77\\200: done after 148 timesteps, total rewards 148, exploration 0.18, memory length 2000\n",
            "Episode 78\\200: done after 89 timesteps, total rewards 89, exploration 0.17, memory length 2000\n",
            "Episode 79\\200: done after 96 timesteps, total rewards 96, exploration 0.15, memory length 2000\n",
            "Episode 80\\200: done after 125 timesteps, total rewards 125, exploration 0.14, memory length 2000\n",
            "Episode 81\\200: done after 101 timesteps, total rewards 101, exploration 0.12, memory length 2000\n",
            "Episode 82\\200: done after 104 timesteps, total rewards 104, exploration 0.11, memory length 2000\n",
            "Episode 83\\200: done after 91 timesteps, total rewards 91, exploration 0.1, memory length 2000\n",
            "Episode 84\\200: done after 142 timesteps, total rewards 142, exploration 0.087, memory length 2000\n",
            "Episode 85\\200: done after 113 timesteps, total rewards 113, exploration 0.078, memory length 2000\n",
            "Episode 86\\200: done after 157 timesteps, total rewards 157, exploration 0.067, memory length 2000\n",
            "Episode 87\\200: done after 117 timesteps, total rewards 117, exploration 0.059, memory length 2000\n",
            "Episode 88\\200: done after 169 timesteps, total rewards 169, exploration 0.05, memory length 2000\n",
            "Episode 89\\200: done after 96 timesteps, total rewards 96, exploration 0.045, memory length 2000\n",
            "Episode 90\\200: done after 98 timesteps, total rewards 98, exploration 0.041, memory length 2000\n",
            "Episode 91\\200: done after 137 timesteps, total rewards 137, exploration 0.036, memory length 2000\n",
            "Episode 92\\200: done after 73 timesteps, total rewards 73, exploration 0.033, memory length 2000\n",
            "Episode 93\\200: done after 73 timesteps, total rewards 73, exploration 0.031, memory length 2000\n",
            "Episode 94\\200: done after 72 timesteps, total rewards 72, exploration 0.029, memory length 2000\n",
            "Episode 95\\200: done after 64 timesteps, total rewards 64, exploration 0.027, memory length 2000\n",
            "Episode 96\\200: done after 74 timesteps, total rewards 74, exploration 0.025, memory length 2000\n",
            "Episode 97\\200: done after 79 timesteps, total rewards 79, exploration 0.023, memory length 2000\n",
            "Episode 98\\200: done after 85 timesteps, total rewards 85, exploration 0.021, memory length 2000\n",
            "Episode 99\\200: done after 91 timesteps, total rewards 91, exploration 0.019, memory length 2000\n",
            "Episode 100\\200: done after 111 timesteps, total rewards 111, exploration 0.017, memory length 2000\n",
            "Episode 101\\200: done after 129 timesteps, total rewards 129, exploration 0.015, memory length 2000\n",
            "Episode 102\\200: done after 200 timesteps, total rewards 200, exploration 0.013, memory length 2000\n",
            "Episode 103\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 104\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 105\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 106\\200: done after 153 timesteps, total rewards 153, exploration 0.01, memory length 2000\n",
            "Episode 107\\200: done after 10 timesteps, total rewards 10, exploration 0.01, memory length 2000\n",
            "Episode 108\\200: done after 9 timesteps, total rewards 9, exploration 0.01, memory length 2000\n",
            "Episode 109\\200: done after 9 timesteps, total rewards 9, exploration 0.01, memory length 2000\n",
            "Episode 110\\200: done after 9 timesteps, total rewards 9, exploration 0.01, memory length 2000\n",
            "Episode 111\\200: done after 8 timesteps, total rewards 8, exploration 0.01, memory length 2000\n",
            "Episode 112\\200: done after 10 timesteps, total rewards 10, exploration 0.01, memory length 2000\n",
            "Episode 113\\200: done after 8 timesteps, total rewards 8, exploration 0.01, memory length 2000\n",
            "Episode 114\\200: done after 9 timesteps, total rewards 9, exploration 0.01, memory length 2000\n",
            "Episode 115\\200: done after 11 timesteps, total rewards 11, exploration 0.01, memory length 2000\n",
            "Episode 116\\200: done after 10 timesteps, total rewards 10, exploration 0.01, memory length 2000\n",
            "Episode 117\\200: done after 10 timesteps, total rewards 10, exploration 0.01, memory length 2000\n",
            "Episode 118\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 119\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 120\\200: done after 167 timesteps, total rewards 167, exploration 0.01, memory length 2000\n",
            "Episode 121\\200: done after 11 timesteps, total rewards 11, exploration 0.01, memory length 2000\n",
            "Episode 122\\200: done after 11 timesteps, total rewards 11, exploration 0.01, memory length 2000\n",
            "Episode 123\\200: done after 11 timesteps, total rewards 11, exploration 0.01, memory length 2000\n",
            "Episode 124\\200: done after 12 timesteps, total rewards 12, exploration 0.01, memory length 2000\n",
            "Episode 125\\200: done after 15 timesteps, total rewards 15, exploration 0.01, memory length 2000\n",
            "Episode 126\\200: done after 13 timesteps, total rewards 13, exploration 0.01, memory length 2000\n",
            "Episode 127\\200: done after 13 timesteps, total rewards 13, exploration 0.01, memory length 2000\n",
            "Episode 128\\200: done after 15 timesteps, total rewards 15, exploration 0.01, memory length 2000\n",
            "Episode 129\\200: done after 17 timesteps, total rewards 17, exploration 0.01, memory length 2000\n",
            "Episode 130\\200: done after 120 timesteps, total rewards 120, exploration 0.01, memory length 2000\n",
            "Episode 131\\200: done after 137 timesteps, total rewards 137, exploration 0.01, memory length 2000\n",
            "Episode 132\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 133\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 134\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 135\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 136\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n",
            "Episode 137\\200: done after 200 timesteps, total rewards 200, exploration 0.01, memory length 2000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50ec1934f222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_replay\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# use experience replay if memory length enough for batching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_replay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m#elif use_replay == False:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-50ec1934f222>\u001b[0m in \u001b[0;36mexp_replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mtarget_q_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# compute next state q value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mq_val_next_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 2972\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}